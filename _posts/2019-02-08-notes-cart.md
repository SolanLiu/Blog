---
layout: post
#标题配置
title:  CART分类树与决策树
#时间配置
date:   2019-02-08 01:08:00 +0800
#大类配置
categories: notes
#小类配置
tag: Machine Learning
---

* content
{:toc}




# 1. CART算法原理

## 1.1 建树

### 1.1.1 CART算法的认识

&emsp;&emsp;Classification And Regression Tree，即分类回归树算法，简称CART算法，它是决策树的一种实现，通常决策树主要有三种实现，分别是ID3算法，CART算法和C4.5算法。  
&emsp;&emsp;CART算法是一种二分递归分割技术，把当前样本划分为两个子样本，使得生成的每个非叶子结点都有两个分支，因此CART算法生成的决策树是结构简洁的二叉树。由于CART算法构成的是一个二叉树，它在每一步的决策时只能是“是”或者“否”，即使一个feature有多个取值，也是把数据分为两部分。在CART算法中主要分为两个步骤：  
&emsp;&emsp;（1）将样本递归划分进行建树过程  
&emsp;&emsp;（2）用验证数据进行剪枝

### 1.1.2 CART算法的原理

- **划分步骤**

&emsp;&emsp;设$x_1,x_2,...,x_n$代表单个样本的$n$个属性，$y$表示所属类别。CART算法通过递归的方式将 维的空间划分为不重叠的矩形。划分步骤大致如下：  
&emsp;&emsp;（1）选一个自变量$x_i$，再选取$x_i$的一个值$v_i$,$v_i$把$n$维空间划分为两部分，一部分的所有点都满足$x_i<v_i$，另一部分的所有点都满足$x_i>v_i$，对非连续变量来说属性值的取值只有两个，即等于该值或不等于该值。  
&emsp;&emsp;（2）递归处理，将上面得到的两部分按步骤（1）重新选取一个属性继续划分，直到把整个$n$维空间都划分完。  
&emsp;&emsp;对于单个样本，只需选取其中一个属性$x_i$即可将其分类，所属类别为$y$。一般情况下，样本个数远大于属性个数，则需要针对每一个属性进行判断分类。

- **划分方法**

&emsp;&emsp;因为CART二分的特性，当训练数据具有两个以上的类别，CART需考虑将目标类别合并成两个超类别，这个过程称为双化。  
&emsp;&emsp;*离散变量*：  
&emsp;&emsp;因为CART树是二叉树，所以对于样本的有N>=3个取值的离散特征的处理时也只能有两个分支，这就要通过组合人为的创建二取值序列并取GiniGain最小者作为树分叉决策点。如某特征值具有['young','middle','old']三个取值,那么二分序列会有如下3种可能性(空集和满集在CART分类中没有意义): 
 
		[(('young',), ('middle', 'old')), (('middle',), ('young', 'old')), (('old',), ('young', 'middle'))]  

&emsp;&emsp;采用CART算法，就需要分别计算按照上述List中的二分序列做分叉时的Gini指数，然后选取产生最小的GINIGain的二分序列做该特征的分叉二值序列参与树构建的递归。如果某特征取值有4个，那么二分序列组合就有7种，5个取值就有15种组合，创建多值离散特征二分序列组合可采用Python的itertools包。  
&emsp;&emsp;*连续变量*:  
&emsp;&emsp;先把连续属性转换为离散属性再进行处理。虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的，如果有N条样本，那么我们有N-1种离散化的方法：$<=v_j$的分到左子树，$>v_j$的分到右子树。计算这N-1种情况下最大的信息增益率。另外，对于连续属性先进行排序（升序），只有在决策属性（即分类发生了变化）发生改变的地方才需要切开，这可以显著减少运算量。  
&emsp;&emsp;（1） 对特征的取值进行升序排序  
&emsp;&emsp;（2） 两个特征取值之间的中点作为可能的分裂点，将数据集分成两部分，计算每个可能的分裂点的GiniGain。优化算法就是只计算分类属性发生改变的那些特征取值    
&emsp;&emsp;（3）选择GiniGain最小的分裂点作为该特征的最佳分裂点（注意，若修正则此处需对最佳分裂点的Gini Gain减去$log2(N-1)/|D|$（N是连续特征的取值个数，D是训练数据数目）  

- **划分标准**

&emsp;&emsp;每个属性的划分按照能减少的杂质的量来进行排序，而杂质的减少量定义为划分前的杂质减去划分后的每个节点的杂质量划分所占比率之和。而杂质度量方法常用Gini指标，假设一个样本共有 类，那么一个节点 的Gini不纯度可定义为
$$ Gini(A) = 1-\sum^C_{i-1}{p_i}^2 $$          
&emsp;&emsp;其中$p_i$表示属于$i$类的概率，当Gini(A)=0时，所有样本属于同类，所有类在节点中以等概率出现时，Gini(A)最大化，此时$C(C-1)/2$。  
&emsp;&emsp;有了上述理论基础，实际的递归划分过程是这样的：如果当前节点的所有样本都不属于同一类或者只剩下一个样本，那么此节点为非叶子节点，所以会尝试样本的每个属性以及每个属性对应的分裂点，尝试找到杂质变量最大的一个划分，该属性划分的子树即为最优分支。  

- **终止条件**

&emsp;&emsp;(1)节点是纯结点，即所有的记录的目标变量值相同    
&emsp;&emsp;(2)树的深度达到了预先指定的最大值  
&emsp;&emsp;(3)混杂度的最大下降值小于一个预先指定的值  
&emsp;&emsp;(4)节点的记录量小于预先指定的最小节点记录量  
&emsp;&emsp;(5)一个节点中的所有记录其预测变量值相同  
&emsp;&emsp;直观的情况，当节点包含的数据记录都属于同一个类别时就可以终止分裂了。  

#### 1.1.3 CART算法简单实例

&emsp;&emsp;下面举个简单的例子，如下图： 

![图像识别的主要过程]({{'/styles/images/machine learning/图1.png' | prepend: site.baseurl }})
    
&emsp;&emsp;在上述图中，属性有3个，分别是有房情况，婚姻状况和年收入，其中有房情况和婚姻状况是离散的取值，而年收入是连续的取值。拖欠贷款者属于分类的结果。  
&emsp;&emsp;假设现在来看有房情况这个属性，那么按照它划分后的Gini指数计算如下：

![图像识别的主要过程]({{'/styles/images/machine learning/图2.png' | prepend: site.baseurl }})
 
&emsp;&emsp;而对于婚姻状况属性来说，它的取值有3种，按照每种属性值分裂后Gini指标计算如下（如果某特征取值有4个，那么二分序列组合就有7种，5个取值就有15种组合，创建多值离散特征二分序列组合可采用Python的itertools包）：

![图像识别的主要过程]({{'/styles/images/machine learning/图3.jpg' | prepend: site.baseurl }})
     
&emsp;&emsp;最后还有一个取值连续的属性，年收入，它的取值是连续的，那么连续的取值采用分裂点进行分裂。如果有N条样本，那么我们有N-1种离散化的方法。另外，对于连续属性先进行排序（升序），只有在决策属性（即分类发生了变化）发生改变的地方才需要切开，这可以显著减少运算量。如下：


![图像识别的主要过程]({{'/styles/images/machine learning/图4.png' | prepend: site.baseurl }})
     
&emsp;&emsp;根据这样的分裂规则CART算法就能完成建树过程。

## 1.2	剪枝

&emsp;&emsp;剪枝的准则是如何确定决策树的规模，可以参考的剪枝思路有以下几个：  
&emsp;&emsp;(1)使用训练集合(Training Set）和验证集合(Validation Set)，来评估剪枝方法在修剪结点上的效用；  
&emsp;&emsp;(2)使用所有的训练集合进行训练，但是用统计测试来估计修剪特定结点是否会改善训练集合外的数据的评估性能，如使用Chi-Square（Quinlan，1986）测试来进一步扩展结点是否能改善整个分类数据的性能，还是仅仅改善了当前训练集合数据上的性能；  
&emsp;&emsp;(3)使用明确的标准来衡量训练样例和决策树的复杂度，当编码长度最小时，停止树增长，如MDL(Minimum Description Length)准则。

### 1.2.1	预剪枝

&emsp;&emsp;预剪枝是根据一些原则及早的停止树增长，如树的深度达到用户所要的深度、节点中样本个数少于用户指定个数、不纯度指标下降的最大幅度小于用户指定的幅度等。

### 1.2.2	后剪枝

&emsp;&emsp;后剪枝则是通过在完全生长的树上剪去分枝实现的，通过删除节点的分支来剪去树节点，可以使用的后剪枝方法有多种，比如：代价复杂性剪枝、最小误差剪枝、悲观误差剪枝等等。  

- **Reduced-Error Pruning(REP,错误率降低剪枝）**

&emsp;&emsp;该剪枝方法考虑将书上的每个节点作为修剪的候选对象，决定是否修剪这个结点由如下步骤组成：  
&emsp;&emsp;1.	删除以此结点为根的子树；  
&emsp;&emsp;2.	使其成为叶子结点；  
&emsp;&emsp;3.	赋予该结点关联的训练数据的最常见分类；  
&emsp;&emsp;4.	当修剪后的树对于验证集合的性能不会比原来的树差时，才真正删除该结点。  
&emsp;&emsp;因为训练集合的过拟合，使得验证集合数据能够对其进行修正，反复进行上面的操作，从底向上的处理结点，删除那些能够最大限度的提高验证集合的精度的结点，直到进一步修剪有害为止(有害是指修剪会减低验证集合的精度)。  
&emsp;&emsp;REP是最简单的后剪枝方法之一，不过在数据量比较少的情况下，REP方法趋于过拟合而较少使用。这是因为训练数据集合中的特性在剪枝过程中被忽略，所以在验证数据集合比训练数据集合小的多时，要注意这个问题。  
&emsp;&emsp;尽管REP有这个缺点，不过REP仍然作为一种基准来评价其它剪枝算法的性能。它对于两阶段决策树学习方法的优点和缺点提供了了一个很好的学习思路。由于验证集合没有参与决策树的创建，所以用REP剪枝后的决策树对于测试样例的偏差要好很多，能够解决一定程度的过拟合问题。

- **Pessimistic Error Pruning(PEP，悲观剪枝）**

&emsp;&emsp;先计算规则在它应用的训练样例上的精度，然后假定此估计精度为二项式分布，并计算它的标准差。对于给定的置信区间，采用下界估计作为规则性能的度量。这样做的结果，是对于大的数据集合，该剪枝策略能够非常接近观察精度，随着数据集合的减小，离观察精度越来越远。该剪枝方法尽管不是统计有效的，但是在实践中有效。  
&emsp;&emsp;PEP采用自顶向下的方式，如果某个非叶子结点符合上面的不等式，就裁剪掉该叶子结点。该算法被认为是当前决策树后剪枝算法中经度比较高的算法之一，但是存在有缺陷。首先，PEP算法是唯一使用Top-Down剪枝策略，这种策略会导致与先剪枝出现同样的问题，将该结点的某子节点不需要被剪枝时被剪掉；另外PEP方法会有剪枝失败的情况出现。  
&emsp;&emsp;虽然PEP方法存在一些局限性，但是在实际应用中表现出了较高的精度。两外PEP方法不需要分离训练集合和验证机和对于数据量比较少的情况比较有利。再者其剪枝策略比其它方法相比效率更高，速度更快。因为在剪枝过程中，树中的每颗子树最多需要访问一次，在最坏的情况下，它的计算时间复杂度也只和非剪枝树的非叶子节点数目成线性关系。  

- **Cost-Complexity Pruning(CCP、代价复杂度)**

&emsp;&emsp;CCP方法包含两个步骤：  
&emsp;&emsp;1.	从原始决策树$T_0$开始生成一个子树序列$T_0,T_1,...,T_n$,其中$T_{i+1}$是从$T_i$总产生，$T_n$为根节点；  
&emsp;&emsp;2.	从子树序列中，根据树的真实误差估计选择最佳决策树。  
&emsp;&emsp;对于分类回归树中的每一个非叶子节点计算它的表面误差率增益值$\alpha$。

$$\alpha = (R(t)-R(T_t))/(|N_t|-1)$$

$N_t$是子树中包含的叶子节点个数，$R(t)$是节点$t$的误差代价。

$$R(t) = r(t)p(t), R(T_t) = \sum R(i)$$
	        
$r(t)$是节点$t$的误差率; $p(t)$是节点$t$上的数据占所有数据的比例，$R(T_t)$是子树$T_t$的误差代价，它等于子树$T_t$上所有叶子节点的误差代价之和。
比如有个非叶子节点$t_4$如图所示：

![图像识别的主要过程]({{'/styles/images/machine learning/图5.png' | prepend: site.baseurl }})
 
&emsp;&emsp;已知所有的数据总共有60条，则节点$t_4$的节点误差代价为：

$$R(t) = r(t)p(t)=\frac{7}{60}$$
 
子树误差代价为：

$$R(T_t) = \sum R(i) = \frac{5}{60}$$
 
以$t_4$为根节点的子树上叶子节点有3个，最终：
 
$$\alpha = (R(t)-R(T_t))/(|N_t|-1) = \frac{1}{60}$$

找到$\alpha$值最小的非叶子节点，令其左右孩子为NULL。  
&emsp;&emsp;剪枝过程特别重要，所以在最优决策树生成过程中占有重要地位。有研究表明，剪枝过程的重要性要比树生成过程更为重要，对于不同的划分标准生成的最大树(Maximum Tree)，在剪枝之后都能够保留最重要的属性划分，差别不大。反而是剪枝方法对于最优树的生成更为关键。
  
# 2.	算法实现  
&emsp;&emsp;算法输入是训练集D，基尼系数的阈值，样本个数阈值，输出是决策树T。我们的算法从根节点开始，用训练集递归的建立CART树。  
&emsp;&emsp;(1)对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归;  
&emsp;&emsp;(2)计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归;  
&emsp;&emsp;(3)计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数，对于离散值和连续值的处理方法和基尼系数的计算见第二节。缺失值的处理方法和上篇的C4.5算法里描述的相同;  
&emsp;&emsp;(4)在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2;  
&emsp;&emsp;(5)对左右的子节点递归的调用1-4步，生成决策树。  

![图像识别的主要过程]({{'/styles/images/machine learning/图6.png' | prepend: site.baseurl }})
 
# 3.	CART算法优缺点

## 3.1 决策树算法的优点：

&emsp;&emsp;1.	简单直观，生成的决策树很直观。  
&emsp;&emsp;2.	基本不需要预处理，不需要提前归一化，处理缺失值。  
&emsp;&emsp;3.	使用决策树预测的代价O(log2m),m为样本数。  
&emsp;&emsp;4.	既可以处理离散值也可以处理连续值，很多算法只是专注于离散值或者连续值。  
&emsp;&emsp;5.	可以处理多维度输出的分类问题。  
&emsp;&emsp;6.	相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释。  
&emsp;&emsp;7.	可以交叉验证的剪枝来选择模型，从而提高泛化能力。  
&emsp;&emsp;8.	对于异常点的容错能力好，健壮性高。 
   
## 3.2 决策树算法的缺点:

&emsp;&emsp;1.	决策树算法非常容易过拟合，导致泛化能力不强，可以通过设置节点最少样本数量和限制决策树深度来改进。  
&emsp;&emsp;2.	决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变，这个可以通过集成学习之类的方法解决。  
&emsp;&emsp;3.	寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优，可以通过集成学习之类的方法来改善。  
&emsp;&emsp;4.	有些比较复杂的关系，决策树很难学习，比如异或，这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。  
&emsp;&emsp;5.	如果某些特征的样本比例过大，生成决策树容易偏向于这些特征，这个可以通过调节样本权重来改善。  